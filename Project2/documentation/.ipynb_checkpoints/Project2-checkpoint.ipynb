{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COS 314 - Project 2\n",
    "\n",
    "|Student nmr|Name          |Date         |\n",
    "|-----------|--------------|-------------|\n",
    "|15015026   |Thomas Scholtz| 23 May 2018 |\n",
    "\n",
    "## Requirements\n",
    "- [Python3](https://docs.python.org/3/)\n",
    "- [numpy](http://www.numpy.org/)\n",
    "\n",
    "### Installation\n",
    "#### Ubuntu\n",
    "```\n",
    "sudo apt-get install python3\n",
    "sudo apt-get install python3-pip  \n",
    "pip install numpy\n",
    "```\n",
    "For convinience I added a makefile entry ```make install``` which should install the pip versions of the required libraries.\n",
    "\n",
    "### Optional Dependencies\n",
    "- [matplotlib](https://matplotlib.org/)\n",
    "```\n",
    "python -mpip install matplotlib\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "A feedforward neural network was written to recognise characters in images. The experiments were done in reverse order (3 to 1) as the first 2 experiments were simple and I wanted to get started with the harder problem, because of that I concluded that 100 hidden units give me the best balance of performance and generalization.\n",
    "\n",
    "## Running\n",
    "To run the experiments you can execute ```make run``` which will run all 3 experiments training each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "To run experiment 1:\n",
    "```python experiment1.py B```\n",
    "Where ```B``` is the character to recognise.\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "### Numerical preprocessing\n",
    "For numerical values the data was normalised to [0-1]. (Min-max normalization). The reason behind doing min-max normalization is the smaller values values improve learning rate as the sigmoid activation function will output values closer to one (1) for most of the values. Eg sigmoid(2)=0.88079707797788.\n",
    "\n",
    "```\n",
    "Example Numerical preprocessing\n",
    "Scaled: 2\tto:\t0.13333333333333333\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 3\tto:\t0.2\n",
    "Scaled: 5\tto:\t0.3333333333333333\n",
    "Scaled: 1\tto:\t0.06666666666666667\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 13\tto:\t0.8666666666666667\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 6\tto:\t0.4\n",
    "Scaled: 6\tto:\t0.4\n",
    "Scaled: 10\tto:\t0.6666666666666666\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "```\n",
    "\n",
    "### Character preprocessing\n",
    "For the characters the data was first converted to lower case, then to the ascii number and finally 97 was subtracted.\n",
    "\n",
    "```\n",
    "Example character preprocessing\n",
    "Scaled: A\tto:\t0\n",
    "Scaled: B\tto:\t1\n",
    "Scaled: C\tto:\t2\n",
    "Scaled: D\tto:\t3\n",
    "Scaled: E\tto:\t4\n",
    "Scaled: F\tto:\t5\n",
    "Scaled: Z\tto:\t25\n",
    "```\n",
    "\n",
    "For the training data a corresponding 0 or 1 was given as the answer for the training data.\n",
    "\n",
    "## Data split\n",
    "For training I used 18000 of the 20000 entries given for training the neural network. The remainder of the data was used to validate the neural network.\n",
    "\n",
    "## Stopping conditions\n",
    "- When a maximum number of epochs is exceeded\n",
    "- When the generalization error, E G , is acceptable\n",
    "\n",
    "## Network Architecture\n",
    "The architecture consists of 17 input neurons (including bias), 100 hidden units and 1 output unit.\n",
    "![NN](Experiment2NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNPAnswerArray(self):\n",
    "    # Char\n",
    "    arr = np.array([[0]])\n",
    "    # Where character is specified. Eg 'A'\n",
    "    if self.lettr in _character_match:\n",
    "        arr[0][0] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "### NN configuration\n",
    "\n",
    "|Epochs|Learning rate|\n",
    "|------|-------------|\n",
    "|30    |0.9          |\n",
    "\n",
    "Momentum tests (Training to recognise the letter 'Z'):\n",
    "\n",
    "#### 0.0 Momentum\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 0.0\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 1.7525568919432206e-22\n",
    "Training success: 98.889%\n",
    "Validation success: 98.901%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_0.0_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.0_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.0_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.0_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.1 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.1_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.1_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.1_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.1_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.2 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.2_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.2_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.2_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.2_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.3 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.3_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.3_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.3_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.3_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.4 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.4_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.4_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.4_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.4_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.5 Momentum\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 0.5\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 2.993767961438769e-21\n",
    "Training success: 98.878%\n",
    "Validation success: 99.25%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_0.5_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.5_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.5_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.5_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.6 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.6_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.6_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.6_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.6_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.7 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.7_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.7_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.7_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.7_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.8 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.8_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.8_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.8_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.8_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 0.9 Momentum\n",
    "![E1_NN_Epoch_Momentum_0.9_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.9_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.9_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.9_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 1.0 Momentum\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 1.0\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 5.0668758290754695e-14\n",
    "Training success: 96.383%\n",
    "Validation success: 95.902%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_1.0_30Epochs_100Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_1.0_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_1.0_30Epochs_100Hiddenunits](Experiment1/E1_NN_Training_Momentum_1.0_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "### NN configuration\n",
    "\n",
    "Based on the previous tests a lower momentum gave better results. So 0.01 was chosen as the momentum and now tests were run to see what learning rate was ideal.\n",
    "\n",
    "|Epochs|Momentum|\n",
    "|------|-------------|\n",
    "|30    |0.01          |\n",
    "\n",
    "#### 0.1 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.1\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 6.717862098369651e-09\n",
    "Training success: 98.2%\n",
    "Validation success: 98.401%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits.png)\n",
    "\n",
    "#### 0.2 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits.png)\n",
    "\n",
    "#### 0.3 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits.png)\n",
    "\n",
    "#### 0.4 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits.png)\n",
    "\n",
    "#### 0.5 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.5\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 2.1486977050637392e-17\n",
    "Training success: 98.711%\n",
    "Validation success: 99.0%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits.png)\n",
    "![E21_NN_Training_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits.png)\n",
    "\n",
    "#### 0.6 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits.png)\n",
    "\n",
    "#### 0.7 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits.png)\n",
    "\n",
    "#### 0.8 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits.png)\n",
    "\n",
    "#### 0.9 Learning rate\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 1.0 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 1.0\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 6.921868880478976e-24\n",
    "Training success: 98.928%\n",
    "Validation success: 99.05%\n",
    "======================================\n",
    "```\n",
    "![E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits](Experiment1/E1_NN_Epoch_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits.png)\n",
    "![E1_NN_Training_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits](Experiment1/E1_NN_Training_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits.png)\n",
    "\n",
    "## Best result\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 1.0\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 6.921868880478976e-24\n",
    "Training success: 98.928%\n",
    "Validation success: 99.05%\n",
    "======================================\n",
    "```\n",
    "The higher learning rate reached the highest training generalization and highest validation success.\n",
    "\n",
    "## Conclusion\n",
    "A low momentum and a high learning rate gave the best generalization results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "|Student nmr|Name|Date|\n",
    "|-----------|----|----|\n",
    "|15015026   |Thomas Scholtz| 23 May 2018|\n",
    "\n",
    "To run experiment 2:\n",
    "```python experiment2.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Numerical preprocessing\n",
    "For numerical values the data was normalised to [0-1]. (Min-max normalization).\n",
    "\n",
    "```\n",
    "Example Numerical preprocessing\n",
    "Scaled: 2\tto:\t0.13333333333333333\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 3\tto:\t0.2\n",
    "Scaled: 5\tto:\t0.3333333333333333\n",
    "Scaled: 1\tto:\t0.06666666666666667\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 13\tto:\t0.8666666666666667\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 6\tto:\t0.4\n",
    "Scaled: 6\tto:\t0.4\n",
    "Scaled: 10\tto:\t0.6666666666666666\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "Scaled: 0\tto:\t0.0\n",
    "Scaled: 8\tto:\t0.5333333333333333\n",
    "```\n",
    "\n",
    "### Character preprocessing\n",
    "For the characters the data was first converted to lower case, then to the ascii number and finally 97 was subtracted.\n",
    "\n",
    "```\n",
    "Example character preprocessing\n",
    "Scaled: A\tto:\t0\n",
    "Scaled: B\tto:\t1\n",
    "Scaled: C\tto:\t2\n",
    "Scaled: D\tto:\t3\n",
    "Scaled: E\tto:\t4\n",
    "Scaled: F\tto:\t5\n",
    "Scaled: Z\tto:\t25\n",
    "```\n",
    "\n",
    "For the training data a corresponding 0 or 1 was given as the answer for the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "For training I used 18000 of the 20000 entries given for training the neural network. The remainder of the data was used to validate the neural network.\n",
    "\n",
    "## Stopping conditions\n",
    "- When a maximum number of epochs is exceeded\n",
    "- When the generalization error, E G , is acceptable\n",
    "\n",
    "## Network Architecture\n",
    "The architecture consists of 17 input neurons (including bias), 100 hidden units and 1 output unit.\n",
    "![NN](Experiment2NN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNPAnswerArray(self):\n",
    "    # Vowels\n",
    "    arr = np.array([[0]])\n",
    "\n",
    "    if self.lettr in \"AEIOU\":\n",
    "        arr[0][0] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "### NN configuration\n",
    "\n",
    "|Epochs|Learning rate|\n",
    "|------|-------------|\n",
    "|30    |0.9          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.0 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.0_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.0_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.0_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.0_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.1 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.1_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.1_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.1_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.1_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.2 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.2_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.2_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.2_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.2_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.3 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.3_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.3_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.3_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.3_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.4 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.4_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.4_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.4_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.4_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.5 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.5_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.5_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.5_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.5_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.6 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.6_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.6_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.6_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.6_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.7 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.7_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.7_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.7_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.7_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.8 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.8_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.8_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.8_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.8_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 0.9 Momentum\n",
    "![E2_NN_Epoch_Momentum_0.9_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.9_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.9_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.9_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "#### 1.0 Momentum\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 1.0\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 0.000190781674932777\n",
    "Training success: 80.667%\n",
    "Validation success: 80.11%\n",
    "======================================\n",
    "```\n",
    "![E2_NN_Epoch_Momentum_1.0_30Epochs_100Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_1.0_30Epochs_100Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_1.0_30Epochs_100Hiddenunits](Experiment2/E2_NN_Training_Momentum_1.0_30Epochs_100Hiddenunits.png)\n",
    "\n",
    "## Best result\n",
    "```\n",
    "======================================\n",
    "This NN is still best:\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 0.0\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 2.9226455776084096e-18\n",
    "Training success: 93.417%\n",
    "Validation success: 91.854%\n",
    "======================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN configuration\n",
    "\n",
    "|Epochs|Momentum|\n",
    "|------|-------------|\n",
    "|30    |0.01         |\n",
    "\n",
    "#### 0.0 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.0_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.0_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.0_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.0_Hiddenunits.png)\n",
    "\n",
    "#### 0.1 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.1\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 0.0005275765203067114\n",
    "Training success: 83.9%\n",
    "Validation success: 84.558%\n",
    "======================================\n",
    "```\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.1_Hiddenunits.png)\n",
    "\n",
    "#### 0.2 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.2_Hiddenunits.png)\n",
    "\n",
    "#### 0.3 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.3_Hiddenunits.png)\n",
    "\n",
    "#### 0.4 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.4_Hiddenunits.png)\n",
    "\n",
    "#### 0.5 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.5\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 8.554018248983182e-12\n",
    "Training success: 91.85%\n",
    "Validation success: 91.654%\n",
    "======================================\n",
    "```\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.5_Hiddenunits.png)\n",
    "\n",
    "#### 0.6 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.6_Hiddenunits.png)\n",
    "\n",
    "#### 0.7 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.7_Hiddenunits.png)\n",
    "\n",
    "#### 0.8 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.8_Hiddenunits.png)\n",
    "\n",
    "#### 0.9 Learning rate\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_0.9_Hiddenunits.png)\n",
    "\n",
    "#### 1.0 Learning rate\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 1.0\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 2.115130722107337e-17\n",
    "Training success: 93.539%\n",
    "Validation success: 91.854%\n",
    "======================================\n",
    "```\n",
    "![E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits](Experiment2/E2_NN_Epoch_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits.png)\n",
    "![E2_NN_Training_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits](Experiment2/E2_NN_Training_Momentum_0.01_30Epochs_100_LR_1.0_Hiddenunits.png)\n",
    "\n",
    "## Best result\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 1.0\n",
    "Momentum: 0.01\n",
    "Epochs: 30\n",
    "Hidden units: 100\n",
    "Training error: 2.115130722107337e-17\n",
    "Training success: 93.539%\n",
    "Validation success: 91.854%\n",
    "======================================\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "|Learning rate|Training sucess|Validation|\n",
    "|-------------|---------------|----------|\n",
    "|0.1          |83.9%          |84.558%   |\n",
    "|0.2          |87.867%        |88.956%   |\n",
    "|0.3          |90.539%        |91.054%   |\n",
    "|0.4          |90.939%        |90.355%   |\n",
    "|0.5          |91.85%         |91.654%   |\n",
    "|0.6          |92.378%        |92.054%   |\n",
    "|0.7          |92.683%        |91.804%   |\n",
    "|0.8          |93.156%        |92.154%   |\n",
    "|0.9          |93.456%        |91.804%   |\n",
    "|1.0          |93.539%        |91.854%   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "From this it would seem that higher momentum values cause too much overfitting leading to a worse outcome.\n",
    "\n",
    "### NN configuration\n",
    "Best result\n",
    "From the knowledge gained in the previous tests, the momentum was reduced and the amount of epochs increased to see if a beter result can be achieved.\n",
    "\n",
    "|Epochs|Momentum|Learning rate|\n",
    "|------|--------|-------------|\n",
    "|50    |0.01    |0.9          |\n",
    "\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 0.01\n",
    "Epochs: 240\n",
    "Hidden units: 150\n",
    "Training error: 0.0003028366944930934\n",
    "Training success: 97.167%\n",
    "Validation success: 96.152%\n",
    "======================================\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3\n",
    "\n",
    "|Student nmr|Name|Date|\n",
    "|-----------|----|----|\n",
    "|15015026   |Thomas Scholtz| 23 May 2018|\n",
    "\n",
    "To run experiment 3:\n",
    "```python experiment3.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "#### Numerical preprocessing\n",
    "For numerical values the data was normalised to [0-1]. (Min-max normalization).\n",
    "\n",
    "#### Character preprocessing\n",
    "For the characters the data was first converted to lower case, then to the ascii number and finally 97 was subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Numerical preprocessing\n",
      "Scaled: 2\tto:\t0.13333333333333333\n",
      "Scaled: 8\tto:\t0.5333333333333333\n",
      "Scaled: 3\tto:\t0.2\n",
      "Scaled: 5\tto:\t0.3333333333333333\n",
      "Scaled: 1\tto:\t0.06666666666666667\n",
      "Scaled: 8\tto:\t0.5333333333333333\n",
      "Scaled: 13\tto:\t0.8666666666666667\n",
      "Scaled: 0\tto:\t0.0\n",
      "Scaled: 6\tto:\t0.4\n",
      "Scaled: 6\tto:\t0.4\n",
      "Scaled: 10\tto:\t0.6666666666666666\n",
      "Scaled: 8\tto:\t0.5333333333333333\n",
      "Scaled: 0\tto:\t0.0\n",
      "Scaled: 8\tto:\t0.5333333333333333\n",
      "Scaled: 0\tto:\t0.0\n",
      "Scaled: 8\tto:\t0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Numerical preprocessing\")\n",
    "\n",
    "data = [2,8,3,5,1,8,13,0,6,6,10,8,0,8,0,8]\n",
    "\n",
    "def _scale(num):\n",
    "        new_min = 0\n",
    "        new_max = 1\n",
    "\n",
    "        new_value = ((num - 0) / (15 - 0)) * (new_max - new_min) + new_min\n",
    "        return new_value\n",
    "\n",
    "for index in data:\n",
    "    print(\"Scaled: \" + str(index) + \"\\tto:\\t\" + str(_scale(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example character preprocessing\n",
      "Scaled: A\tto:\t0\n",
      "Scaled: B\tto:\t1\n",
      "Scaled: C\tto:\t2\n",
      "Scaled: D\tto:\t3\n",
      "Scaled: E\tto:\t4\n",
      "Scaled: F\tto:\t5\n",
      "Scaled: Z\tto:\t25\n"
     ]
    }
   ],
   "source": [
    "print(\"Example character preprocessing\")\n",
    "\n",
    "data = ['A','B','C','D','E','F','Z']\n",
    "\n",
    "def _char(lettr):\n",
    "    return ord(lettr.lower()) - 97\n",
    "\n",
    "for index in data:\n",
    "    print(\"Scaled: \" + str(index) + \"\\tto:\\t\" + str(_char(index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "For training I used 18000 of the 20000 entries given for training the neural network. The remainder of the data was used to validate the neural network.\n",
    "\n",
    "## Stopping conditions\n",
    "- When a maximum number of epochs is exceeded\n",
    "- When the generalization error, E G , is acceptable\n",
    "\n",
    "## Network Architecture\n",
    "The architecture consists of 17 input neurons (including bias), 150 hidden units and 26 output units.\n",
    "![NN](Experiment3NN.png)\n",
    "The hidden layer was adjusted in the next section to see if convergence can be improved.\n",
    "\n",
    "## Tests\n",
    "To test for the number of hidden units that give the best convergence. I tested an increasing number of hidden units and recorded the error rate and the percentage success rate.\n",
    "\n",
    "#### NN configuration\n",
    "|Epochs|Momentum|Learning rate|\n",
    "|------|--------|-------------|\n",
    "|50    |0.03     |0.9          |\n",
    "\n",
    "![NN_50Epochs_50Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_50Hiddenunits.png)\n",
    "![NN_50Epochs_100Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_100Hiddenunits.png)\n",
    "![NN_50Epochs_150Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_150Hiddenunits.png)\n",
    "![NN_50Epochs_200Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_200Hiddenunits.png)\n",
    "![NN_50Epochs_250Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_250Hiddenunits.png)\n",
    "![NN_50Epochs_300Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_300Hiddenunits.png)\n",
    "![NN_50Epochs_350Hiddenunits](50Epochs_Momentum_03_Experiment3/NN_50Epochs_350Hiddenunits.png)\n",
    "\n",
    "After this I concluded that with this configuration 100 to 190 hidden units appear to give the best convergance rates.\n",
    "\n",
    "However with 50 Epochs I only reach 86% success rate. So I tried using 150 hidden unints with 240 epochs to see if I can get my success rate up.\n",
    "\n",
    "The configuration values was adjusted:\n",
    "\n",
    "|Epochs|Momentum|Learning rate|\n",
    "|------|--------|-------------|\n",
    "|240   |0.01     |0.9         |\n",
    "\n",
    "![NN_240Epochs_150_M_001_Hiddenunits](NN_240Epochs_150_M_001_Hiddenunits.png)\n",
    "\n",
    "```\n",
    "======================================\n",
    "Network stats: \n",
    "Learning rate: 0.9\n",
    "Momentum: 0.01\n",
    "Epochs: 240\n",
    "Hidden units: 150\n",
    "Training error: 0.0037070822341378953\n",
    "Training success: 92.456%\n",
    "Validation success: 91.154%\n",
    "======================================\n",
    "```\n",
    "\n",
    "Increasing the number of epochs got me to 92% success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
